概述：
Senario, Service, Storage, Scale
题型： a) 设计系统 / 功能，缺描述
	b) Trouble shooting 一般难以准备需要经验
针对worst case scenario 进行优化，一般4s中storage部分尤其重要。
RPS: request per sec, QPS是RPS的大约十倍两级；
QPS的peak经验值大概在2到9倍
QPS 决定了系统存储系统的选择，进而决定了系统的体系大小和复杂程度
Service 分解能力体现了由大化小的工程能力

数据库缓存：
Cache本地的静态文件的文件名是哈希值，服务器端改变某文件后哈希值会变，会导致重新下载
cache可以设定过期时间（memchached），cache有可能被淘汰，类似os的机制，比如LRU，locality。
cookies 不是cache
cache需要严格避免脏数据（cache hit，并且和database不一致）
web cache 的hit rate超过90
cookie类似于通行证
session_key是服务器端生成绝对唯一的密钥性质的字符串
serialization是指把一个object翻译成字符串进行存储
sql的schema是metadata性质的，而nosql的row_key和col_key并不是metadata

一致性哈希：
Consistent hashing very important for scaling since horizontal sharding relies on it.
consisitent hashing 很多时候是用红黑树实现的 virtual node 的存储
MySQL 的 Master-slave replica 机制中，所有的写操作需经过master，读操作可以由 slave 处理
在某个极小的时间段内，replica无法保证数据的完全一致性

分布式文件系统：
GFS, 存储量级是PB级的
MapReduce 结构中的slave节点间存储的数据没有交集，类似于partition的关系
存储大文件（100TB）用chunk代替block
可以把每台chunkserver的响应的offset存在ChunkServer上
开始的方案可以是“糙快猛”的，之后进行优化
文件写入时，由client分拆文件，之后与master通信确定每个chunk的位置，然后将该chunk直接写入对应chunkserver
分布式文件系统，不提供修改操作，而是采用读旧，写新，弃旧的过程
master server只负责维护以及服务文件的metadata的查询请求，这样可以减轻master server的负担
Scale是GFS的重中之重，实现PAXOS
一般是读数据的时候进行文件数据校验（checksum检验）
Heart beat, master periodically polls chunk servers and check if they are ok, OR, chunkserver peroidically report to master server that it's ok. Most systems use the latter fashion.

Crawler
每个网页需要反复的crawl：假设 1 trillion 个网页 （节点）
	time efficiency: 一周遍历一次所有的网页（节点），平均每秒大约1.6m个网页
	space efficiency: 网页文件，假设每个网页10k，一万亿个网页有1PB的数据
Crawler基本上是个consumer / producer的模型，scale up的通行方法是多线程
multithreading时，sleep会造成资源浪费，condition variable和mutex类似 （0/1锁），semaphore（多重锁）
分布式的crawler，用一台db server存储任务队列，同时任务队列里的url不会被反复删除添加，只是修改state 和 available time。
任务队列中available time用来控制抓取频率：只有 available time 时间点达到了的url会被抓取。
Slow select：当任务队列数据库增长到巨大体积时，sql执行会非常缓慢，采取horizontal sharding + scheduler的办法，比如每个数据表各取100个url返还给crawler machine
In the task queue db, the available time grows to 2 times longer if a crawler failed to access it. So in this way, a "real unavailable" page's available time would grow in a exponential speed to almost not available until forever.
And for fast updating  pages, we cut the next available time in half, to make sure it's accessed frequent enough.
Priority can be arranged according to available times.
Quota can be used in case of a giant page consumes all resource of a crawler

Typeahead
系统关键在于 a) 数据来源与数据请求 b) 性能
google为例，每次在搜索栏里面打一个字符就可能会使用一次typeahead系统；
用db查找例如SELECT * FROM sometable WHERE something LIKE {blah} ORDERED BY hitcount, 会非常慢，原因可以直接看SQL语句
优化思路可以是空间换时间：存prefix对应的keyword列表，把范围查找变为定点访问；
继续优化思路，prefix很自然的使用到trie tree结构；在trie中，可以把平常的end flag存为该词的hit count以方便排序；
query过程中需要先查找cache，避免硬盘查找
用户搜索的keyword需要快速，无逻辑的写入log，然后再统计整个log以确定关键词
Typeahead需要的是极为迅速的反馈，而不是准确性和时效性，所以甚至可以每两周统计一次log
如果需要系统更快，可以再浏览器端做文章，比如浏览器端的缓存 
由于Trie结构非常大（26^n）需要用到consistent hashing来分开存trie；具体是对输入的keyword进行hashing

MapReduce
mapper和reducer机器由master机器管理
map reduce 不做aggregation 因为在最后的合并步骤会产生瓶颈
master负责split输入文件
